# -*- coding: utf-8 -*-
"""KSD-F2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/thivyaar/ksd-f2.932e4b20-4024-4323-aeb3-8367f7b98232.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250709/auto/storage/goog4_request%26X-Goog-Date%3D20250709T075250Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9e9c1e71335062a1b1b65321e3fab7f899d34e1e8d7f6cd559fbee3ecf361e57121a343c7e271e62311acd89a0e2027d9778dbd3c57ad0340fa33760c53439e6ed7a6a755a1a690e9e78e72ad6039e9e900ff966651faf26f194e5507cec5a9b6c68356c161d8d60b33f381d62a9147993a69a77b6d03ea2807eb540dc45238fe8ba04dcc28b31571a8a3c19b4fd054a39a80c7989ea38feae9011fa8d79129ae384452ffa9895dea3da705f390b9fe7aac48aa6b987d2ad6449bb6b3f0f5d4292008772e9e1c65067626dae6a534c26b42d159bec416eebfac83913596ff7074a307f79507e3d4663d4659d2a9cba580bae6b25755049024930db3f56c64c67
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

thivyaar_kds_dataset_path = kagglehub.dataset_download('thivyaar/kds-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import os
import torch
import torchvision
import torch.nn as nn
import xml.etree.ElementTree as ET
from torchvision.models.resnet import resnet50
from torch.utils.data import Dataset, DataLoader
from torchvision.utils import draw_bounding_boxes
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.transforms.functional import to_tensor, to_pil_image
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from torchvision.transforms import functional as TF
from albumentations.pytorch import ToTensorV2
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import torchvision.ops as ops
import albumentations as A
import seaborn as sns
from PIL import Image
import numpy as np
import random
import cv2

class KidneyDataset(Dataset):
    def __init__(self, img_path, ann_dir, transform=None):
        self.img_paths = [
            os.path.join(img_path, f)
            for f in os.listdir(img_path)
            if f.endswith(('.jpg', '.png', '.jpeg'))
        ]
        self.ann_dir = ann_dir
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):

           image = self.load_image(self.img_paths[idx])

           annotation_file = os.path.basename(self.img_paths[idx])
           annotation_file = annotation_file.replace('.jpg', '.xml').replace('.png', '.xml').replace('.jpeg', '.xml')
           annotation_path = os.path.join(self.ann_dir, annotation_file)
           if os.path.exists(annotation_path):
            target = self.parse_annotation(annotation_path)
           else:
            target = {
                "boxes": torch.zeros((0, 4), dtype=torch.float32),
                "labels": torch.tensor([0], dtype=torch.int64)
            }


           img_np = np.array(image)

           if self.transform:
               transformed = self.transform(image=img_np, bboxes=target['boxes'], labels=target['labels'])
               image = transformed['image']
               target['boxes'] = torch.tensor(transformed['bboxes'], dtype=torch.float32)
               target['labels'] = torch.tensor(transformed['labels'], dtype=torch.int64)
           else:
            image = to_tensor(image)
           return image, target

    def load_image(self, path):
            from PIL import Image
            return Image.open(path)

    def parse_annotation(self, annotation_path):
        """Parses the XML annotation file and returns the target dictionary."""
        tree = ET.parse(annotation_path)
        root = tree.getroot()

        boxes = []
        labels = []

        for obj in root.findall('object'):
            label = obj.find('name').text
            bndbox = obj.find('bndbox')
            xmin = int(bndbox.find('xmin').text)
            ymin = int(bndbox.find('ymin').text)
            xmax = int(bndbox.find('xmax').text)
            ymax = int(bndbox.find('ymax').text)
            boxes.append([xmin, ymin, xmax, ymax])
            if label == "Stone":
              labels.append(1)
            else:
                labels.append(0)
        # Convert to tensors
        target = {
            "boxes": boxes,
            "labels": labels
        }

        return target

def dataset_transforms(image, bboxes, labels):
    image = torchvision.transforms.functional.to_tensor(image)
    return {'image': image, 'bboxes': bboxes, 'labels': labels}

class CBAM(nn.Module):
    def __init__(self, channels, reduction_ratio=16):
        super(CBAM, self).__init__()

        # Channel Attention
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // reduction_ratio, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(channels // reduction_ratio, channels, kernel_size=1),
            nn.Sigmoid()
        )

        # Spatial Attention
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )

        self.channel_attention[3].bias.data.fill_(1)
        self.spatial_attention[0].bias.data.fill_(1)

    def forward(self, x):
        channel_att = self.channel_attention(x)
        x = channel_att * x  #
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_att = self.spatial_attention(torch.cat([avg_out, max_out], dim=1))

        return spatial_att * x

class AttentionResNetBackbone(nn.Module):
  """Resnet50 backbone with CBAM attention."""
  def __init__(self, backbone):
        super(AttentionResNetBackbone, self).__init__()
        self.backbone = nn.Sequential(*list(backbone.children())[:-2])
        self.cbam = CBAM(channels=backbone.fc.in_features)
        self.out_channels = backbone.fc.in_features

  def forward(self, x):
        x = self.backbone(x)
        x = self.cbam(x)
        return {"0": x}

classes =["Normal", "Stone"]

train_dataset = KidneyDataset(
    img_path="/kaggle/input/kds-dataset/Dataset/Train/images",
    ann_dir="/kaggle/input/kds-dataset/Dataset/Train/labels",
    transform=dataset_transforms
)

test_dataset = KidneyDataset(
    img_path = "/kaggle/input/kds-dataset/Dataset/Test/images",
    ann_dir = "/kaggle/input/kds-dataset/Dataset/Test/labels"
)

len(train_dataset), len(test_dataset)

from collections import Counter
class_counts = Counter(label.item() for sample in train_dataset for label in sample[1]['labels'])

for label, count in class_counts.items():
    print(f"Class {label}: {count} training samples")

sample = train_dataset[5]
img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)
plt.imshow(draw_bounding_boxes(
    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4
).permute(1, 2, 0))
plt.show()

#Backbone
n_classes = 2
resnet_backbone = resnet50(pretrained=True)
attention_backbone = AttentionResNetBackbone(resnet_backbone)

#Anchor boxes
anchor_gen = AnchorGenerator(
    sizes=((16, 32, 64, 128, 256, 512, 768),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

roi_pooler = torchvision.ops.MultiScaleRoIAlign(
    featmap_names=["0"],
    output_size=7,
    sampling_ratio=2
)

model = FasterRCNN(
    backbone=attention_backbone,
    num_classes=n_classes,
    rpn_anchor_generator=anchor_gen,
    box_roi_pool=roi_pooler
)

def collate_fn(batch):
    return tuple(zip(*batch))

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = model.to(device)

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)

num_epoch = 40
train_losses = []
for epoch in range(num_epoch):
    model.train()
    train_loss = 0.0

    for images, targets in train_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        loss = sum(loss for loss in loss_dict.values())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    avg_train_loss = train_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}")

save_path = "/kaggle/working/kidney_stone_model.pth"
torch.save(model.state_dict(), save_path)
print(f"Model saved to {save_path}")

import os
import numpy as np
import cv2
import torch
import torchvision.ops as ops
import matplotlib.pyplot as plt
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

model.eval()
save_path = "/content/Outputs"
os.makedirs(save_path, exist_ok=True)

# Initialize lists
true_labels = []
pred_labels = []
predictions = []
stone_sizes = []
all_final_labels = []
threshold = 0.8
nms_iou_threshold = 0.4

# Iterate over test dataset
for i in range(len(test_dataset)):
    img, target = test_dataset[i]
    img = img.to(device)

    with torch.no_grad():
        prediction = model([img])
        pred = prediction[0]
        keep = ops.nms(pred['boxes'], pred['scores'], iou_threshold=nms_iou_threshold)
        filtered_boxes = pred['boxes'][keep]
        filtered_scores = pred['scores'][keep]
        filtered_labels = pred['labels'][keep]

        high_confidence = filtered_scores >= threshold
        final_boxes = filtered_boxes[high_confidence]
        final_scores = filtered_scores[high_confidence]
        final_labels = filtered_labels[high_confidence]

        # Classification labels
        if len(final_scores) > 0:
            predictions.append("Stone")
            pred_labels.append(1)
        else:
            predictions.append("Normal")
            pred_labels.append(0)

        if len(target['labels']) > 0 and any(label == 1 for label in target['labels']):
            true_labels.append(1)
        else:
            true_labels.append(0)

        all_final_labels.extend(final_labels.cpu().numpy())

        img_int = (img * 255).clamp(0, 255).byte().cpu()
        img_int_rgb = img_int.repeat(3, 1, 1) if img_int.shape[0] == 1 else img_int[:3]

        # Draw bounding boxes with labels
        if len(final_boxes) > 0:
            annotated_img = draw_bounding_boxes(
                img_int_rgb,
                final_boxes.cpu(),
                labels=[f"Stone ({score:.2f})" for score in final_scores.cpu().tolist()],
                width=4
            )
        else:
            annotated_img = img_int_rgb

        pil_img = to_pil_image(annotated_img)
        img_filename = f"{save_path}/annotated_image_{i}.png"
        pil_img.save(img_filename)

        img_cv2 = np.array(to_pil_image(img_int_rgb))

        # If stones are detected, process for stone size
        if len(final_boxes) > 0:
            img_with_size = img_cv2.copy()
            for j, box in enumerate(final_boxes):
                x_min, y_min, x_max, y_max = map(int, box.cpu().numpy())
                cropped_stone = img_cv2[y_min:y_max, x_min:x_max]
                gray_stone = cv2.cvtColor(cropped_stone, cv2.COLOR_RGB2GRAY)
                _, binary_stone = cv2.threshold(gray_stone, 200, 255, cv2.THRESH_BINARY)

                stone_size = np.sum(binary_stone == 255)
                stone_sizes.append(stone_size)

                crop_filename = f"{save_path}/cropped_stone_{i}.png"
                cv2.imwrite(crop_filename, binary_stone)

                print(f"Image {i} - Detected Stone Size (white pixels): {stone_size}")

                cv2.rectangle(img_with_size, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                cv2.putText(img_with_size, f"Size: {stone_size}", (x_min, y_min - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            img_annotated = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            img_with_size = cv2.cvtColor(img_with_size, cv2.COLOR_RGB2BGR)

            fig, axs = plt.subplots(1, 2, figsize=(10, 5))
            axs[0].imshow(img_annotated)
            axs[0].set_title("Detected Stones")
            axs[0].axis('off')

            axs[1].imshow(img_with_size)
            axs[1].set_title("Detected Stones with Sizes")
            axs[1].axis('off')

            plt.show()

        else:
            plt.figure(figsize=(5, 5))
            plt.imshow(pil_img)
            plt.title("Normal - No Stones Detected")
            plt.axis('off')
            plt.show()

true_labels = np.array(true_labels)
pred_labels = np.array(pred_labels)
stone_sizes = np.array(stone_sizes)

accuracy = accuracy_score(true_labels, pred_labels)
cm = confusion_matrix(true_labels, pred_labels)
report = classification_report(true_labels, pred_labels)
print(f"Accuracy: {accuracy:.4f}")
print(f"Confusion Matrix:\n{cm}")
print(f"Classification Report:\n{report}")

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=set(true_labels), yticklabels=set(true_labels))
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

min_len = min(len(true_labels), len(pred_labels))
true_labels = true_labels[:min_len]
pred_labels = pred_labels[:min_len]

# Compute accuracy
accuracy = accuracy_score(true_labels, pred_labels)
print(f"Accuracy: {accuracy:.4f}")

plt.plot(range(1, num_epoch+1), train_losses, label="Train Loss", marker='o')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()
plt.grid(True)
plt.show()



